\documentclass{article}
\usepackage{myVignette}
\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}
%%\VignetteIndexEntry{Comparisons of Least Squares calculation speeds}
%%\VignetteDepends{Matrix}
\usepackage{/u/maechler/R/D/r-patched/64-linux-inst/share/texmf/Sweave}
\begin{document}

\setkeys{Gin}{width=\textwidth}
\title{Comparing Least Squares Calculations}
\author{Douglas Bates\\R Development Core Team\\\email{Douglas.Bates@R-project.org}}
\date{\today}
\maketitle
\begin{abstract}
  Many statistics methods require one or more least squares problems
  to be solved.  There are several ways to perform this calculation,
  using objects from the base R system and using objects in the
  classes defined in the \code{Matrix} package.
  
  We compare the speed of some of these methods on a very small
  example and on a example for which the model matrix is large and
  sparse.
\end{abstract}

\section{Linear least squares calculations}
\label{sec:LeastSquares}

Many statistical techniques require least squares solutions
\begin{equation}
  \label{eq:LeastSquares}
  \widehat{\bm{\beta}}=
  \arg\min_{\bm{\beta}}\left\|\bm{y}-\bX\bm{\beta}\right\|^2
\end{equation}
where $\bX$ is an $n\times p$ model matrix ($p\leq n$), $\bm{y}$ is
$n$-dimensional and $\bm{\beta}$ is $p$ dimensional.  Most statistics
texts state that the solution to (\ref{eq:LeastSquares}) is
\begin{equation}
  \label{eq:XPX}
  \widehat{\bm{\beta}}=\left(\bX\trans\bX\right)^{-1}\bX\trans\bm{y}
\end{equation}
when $\bX$ has full column rank (i.e. the columns of $\bX$ are
linearly independent) and all too frequently it is calculated in
exactly this way.


\subsection{A small example}
\label{sec:smallLSQ}

As an example, let's create a model matrix, \code{mm}, and corresponding
response vector, \code{y}, for a simple linear regression model using
the \code{Formaldehyde} data.
\begin{Schunk}
\begin{Sinput}
> data(Formaldehyde)
> str(Formaldehyde)
\end{Sinput}
\begin{Soutput}
`data.frame':	6 obs. of  2 variables:
 $ carb  : num  0.1 0.3 0.5 0.6 0.7 0.9
 $ optden: num  0.086 0.269 0.446 0.538 0.626 0.782
\end{Soutput}
\begin{Sinput}
> print(mm <- cbind(1, Formaldehyde$carb))
\end{Sinput}
\begin{Soutput}
     [,1] [,2]
[1,]    1  0.1
[2,]    1  0.3
[3,]    1  0.5
[4,]    1  0.6
[5,]    1  0.7
[6,]    1  0.9
\end{Soutput}
\begin{Sinput}
> print(y <- Formaldehyde$optden)
\end{Sinput}
\begin{Soutput}
[1] 0.086 0.269 0.446 0.538 0.626 0.782
\end{Soutput}
\end{Schunk}
Using \code{t} to evaluate
the transpose, \code{solve} to take an inverse, and the \code{\%*\%}
operator for matrix multiplication, we can translate \ref{eq:XPX} into
the \Slang{} as
\begin{Schunk}
\begin{Sinput}
> solve(t(mm) %*% mm) %*% t(mm) %*% y
\end{Sinput}
\begin{Soutput}
            [,1]
[1,] 0.005085714
[2,] 0.876285714
\end{Soutput}
\end{Schunk}

On modern computers this calculation is performed so quickly that it cannot
be timed accurately in \RR{}
\begin{Schunk}
\begin{Sinput}
> system.time(solve(t(mm) %*% mm) %*% t(mm) %*% y, gc = TRUE)
\end{Sinput}
\begin{Soutput}
[1] 0 0 0 0 0
\end{Soutput}
\end{Schunk}
and it provides essentially the same results as the standard
\code{lm.fit} function that is called by \code{lm}.
\begin{Schunk}
\begin{Sinput}
> dput(c(solve(t(mm) %*% mm) %*% t(mm) %*% y))
\end{Sinput}
\begin{Soutput}
c(0.00508571428571368, 0.876285714285715)
\end{Soutput}
\begin{Sinput}
> dput(lm.fit(mm, y)$coefficients)
\end{Sinput}
\begin{Soutput}
structure(c(0.00508571428571436, 0.876285714285714), .Names = c("x1", 
"x2"))
\end{Soutput}
\end{Schunk}
%$

\subsection{A large example}
\label{sec:largeLSQ}

For a large, ill-conditioned least squares problem, such as that
described in \citet{koen:ng:2003}, the literal translation of
(\ref{eq:XPX}) does not perform well.
\begin{Schunk}
\begin{Sinput}
> library(Matrix)